<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="shortcut icon" href="../assets/billiam_logo.png">
        <link rel="apple-touch-icon" href="../assets/billiam_logo.png">
        <link rel="stylesheet" href="../assets/style/main.css">
        <link rel="stylesheet" href="../assets/style/article.css">
        <link rel="stylesheet" href="../assets/highlight/atom-one-dark.css">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
        <script src="../assets/highlight/highlight.js"></script>
        <script>hljs.highlightAll();</script>
        <script>history.scrollRestoration = "manual"</script>
        <title>Billiam | Water System</title>
    </head>
    <body>
        <!-- Navbar -->
        <nav class="navbar" id="navbar">
            <img class="navbar-logo" src="../assets/billiam_logo_full.png">
            <div class="navbar-items">
                <a class="navbar-item" href="../index.html#home">Home</a>
                <a class="navbar-item" href="../index.html#about">About</a>
                <a class="navbar-item" href="../projects.html">Projects</a>
            </div>
            <div class="navbar-items aligned-right">
                <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://github.com/billiam-dev">
                    <img class="navbar-image enlarge-on-hover" src="../assets/github_icon_white.png" alt="Github">
                </a>
                <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/william-barber-9a0841278/">
                    <img class="navbar-image enlarge-on-hover" src="../assets/linkedin_icon_white.png" alt="LinkedIn">
                </a>
                <a class="navbar-item" target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/@BilliamDev">
                    <img class="navbar-image enlarge-on-hover" src="../assets/youtube_icon_white.png" alt="Youtube">
                </a>
            </div>
        </nav>
        <!-- Article -->
        <section class="hero is-fullheight has-gradient-background below-navbar">
            <div class="hero-body">
                <div class="container has-gray-background has-drop-shadow has-border">
                    <div class="container">
                        <h2>Abstract</h2>
                        <p1>
                            This is a brief overview of my Water System for Unity, summarising the techniques used in making a convincing ocean simulation for real-time games.
                            As it turns out, building a convincing ocean system is an extremely complicated task, especially for someone who had never used hlsl, nor even heard of a compute shader.
                            By far the most daunting task for me was working out where to even start with each individual component in the system, and that's not to say there aren't several more features that could be implemented.
                            If by chance you are reading this and looking to build your own water system, I've linked the resources I used throughout and at the bottom of the page.
                        </p1>
                        <h2>Sections</h2>
                        <ul>
                            <li>
                                <a class="hide-link" href="#setup">Setup</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#wave-displacement">Wave Displacement</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#water-shading">Water Shading</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#dynamic-detail">Dynamic Detail</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#underwater">Underwater Rendering</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#caustics">Caustics</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#godrays">Godrays</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#cutouts">Water Cutouts</a>
                            </li>
                            <li>
                                <a class="hide-link" href="#sources">Sources</a>
                            </li>
                        </ul>
                    </div>
                    <div class="container">
                        <a class="anchor" id="setup"></a>
                        <h2>Setup</h2>
                        <p1>
                            Entering this project, I had previously made water effects using a subdivided plane and Unity's shader graph producing an acceptable but mediocre result.
                            For this project, however, I wanted to know how water is handled in games like Sea of Theives and Subnautica, and how to integrate it in a clean and usable manor into Unity.
                            So what better place to look than Unity's own water system for the High Definition Render Pipeline?
                            <br><br>
                            Though I was building my water system for the Universal Render Pipeline (which would create an extra challenge later), HDRP is the best example of how Unity themselves would approach this problem.
                            HDRP simplifies creating and managing oceans by having just one 'WaterSurface' component, which tells the backend to render the surface, caustics and underwater effects.
                            This avoids the need for mesh renderers, renderer features or extra components spreading the functionality of the water over several objects and assets.
                            Taking a peak at the source code, we see that HDRP updates and renders it's water by utilizing command buffers.
                            <br><br>
                            Command buffers are essentially a sequence of rendering instructions that you create and then send to the GPU.
                            Then, they can be executed again and again without having to reconstruct and send the buffer, among other performance benefits.
                            <br>
                            We can hook into URP's per-camera rendering callback like so:
                            <pre><code class="has-border">RenderPipelineManager.beginCameraRendering += RenderSurface;</code></pre>
                            And fetch a command buffer using CommandBufferPool.Get():
                            <pre><code class="has-border">void RenderSurface(ScriptableRenderContext context, Camera camera)
{
    CommandBuffer commandBuffer = CommandBufferPool.Get("Water CMD");
    
    UpdateSurface(waterSurface, commandBuffer, context, camera);
    
    context.ExecuteCommandBuffer(commandBuffer);
    context.Submit();

    commandBuffer.Clear();
    CommandBufferPool.Release(commandBuffer);
}</code></pre>
                            The UpdateSurface() is used to queue compute shaders and Scriptable Render Passes; including the FFT, caustics, surface passes and underwater post processing.
                        </p1>
                    </div>
                    <div class="container">
                        <a class="anchor" id="wave-displacement"></a>
                        <h2>Wave Displacement</h2>
                        <p1>
                            In my previous water shader mentioned above, I displaced the ocean vertices using the sum of four <a href="https://en.wikipedia.org/wiki/Trochoidal_wave">Gerstner Waves</a>, each with different amplitudes, frequencies and wavelengths.
                            While this looked acceptable over a small area, observing the water from above or below gives rise to noticeable repeating, or tiling, patterns.
                            This problem is visable in so many games' oceans, since we can't simply tell the GPU to render the millions of waveforms present in a real ocean without it totally exploding.
                            So how do games like Sea of Thieves create their stunning wave visuals? The secret is the <i>Fast Fourier Transform</i>.
                        </p1>
                        <figure>
                            <img class="has-border" src="../assets/projects/water-system/sea_of_thieves_ocean.webp">
                            <figcaption>Sea of Thieves Ocean</figcaption>
                        </figure>
                        <p>
                            You might have already heard of the Fast Fourier Transform and it's uses in ocean rendering.
                            It has been used in dozens of games and movies after it was proposed in the 2004 paper <a href="https://people.computing.clemson.edu/~jtessen/reports/papers_files/coursenotes2004.pdf">Simulating Ocean Water</a> by Jerry Tessendorf.
                            I first learned of it from a source I am much more familiar with; Acerola's 2023 video <a href="https://www.youtube.com/watch?v=yPfagLeUa7k">I Tried Simulating The Entire Ocean</a>.
                            Both were extremely useful in understanding the wave displacement and water shading techniques below.
                            <br><br>
                            The thesis behind the Fourier Transform is that any periodic waveform can be built from a sum of sinusoids - sine and cosine waves.
                            The purpose of Fourier Transform is to take a waveform and decompose it into that sum of sinusoids, see: <a href="https://www.thefouriertransform.com/#introduction">here</a> to learn more about the Fourier Transform.
                            <br><br>
                            It's output, then, can be viewed as a wave graph in the frequency domain which, as opposed to describing how amplitude of a wave changes over time, describes how the amplitude of a wave changes with a range of frequencies.
                            For example, for a wave of frequency ~30hz, so would see a spike in the frequency domain at ~30hz. The size of the spike is proportional to it's amplitude, as we see in the figure below.
                            <figure class="image small">
                                <img src="../assets/projects/water-system/time_domain_vs_frequency_domain.ppm">
                                <figcaption>Time domain vs frequency domain. Figure from <a href="https://www.researchgate.net/figure/Time-domain-graph-and-frequency-domain-graph-Take-034-013-When-there-is-only_fig1_342559582">here</a></figcaption>
                            </figure>
                            The neat part is this operation is reversible, known as the Inverse Fourier Transform.
                            Performing an IFT on a signal in the frequency domain converts it into a waveform in the time domain, which we can sample in our shader.
                            To generate a wave spectra, we can actually use real-world wave data such as the Phillips spectrum, which is used by both HDRP and Sea of Theives.
                            However, the developers behind <i>Atlas</i> in their 2021 GDC talk on water simulation recommends Duel fetch limited JONSWAP spectra for more artistic control over the waves.
                            <br><br>
                            Now to perform the IFT on our JONSWAP spectra, there are several algorithms we can choose from.
                            We, of course, want to go with the fastest one available creatively known as the Fast Fourier Transform (FFT).
                            The FFT uses a 'Butterfly Algorithm' to recursively split the wave data in half avoiding unnecessary calculations, see <a href="https://antoniospg.github.io/UnityOcean/OceanSimulation.html">here</a> for detail on the mathematics.
                            <br><br>
                            To summarise all of this stuff in a sentence: we generate and progress a frequency domain wave spectrum, then perform the IFT over a set of discrete points (texture coords) to attain usable displacement values for our shader.
                            We can derrive a slope map for normal vectors from the displacement map, and the resulting textures look like this:
                        </p>
                        <div class="columns">
                            <div class="column">
                                <figure class="image tiny">
                                    <img src="../assets/projects/water-system/wave_displacement.gif">
                                    <figcaption>Displacement Map</figcaption>
                                </figure>
                            </div>
                            <div class="column">
                                <figure class="image tiny">
                                <img src="../assets/projects/water-system/wave_slope.gif">
                                    <figcaption>Slope Map</figcaption>
                                </figure>
                            </div>
                        </div>
                        <p>
                            However, even with very detailed wave spectra, we have not completely erradicated tiling.
                            So, we actually compute several wave spectra simultaneously and scale them differently over the ocean surface.
                            The idea is to distort the details of each spectrum by the other spectra, reducing the "same-y-ness" of tiling artefacts.
                        </p>
                        <img class="has-border" src="../assets/projects/water-system/fft_waves.png">
                        <p>
                            Last in this section is a few additional notes I came across along the way:
                            <br>
                            You change the resolution of your FFTs at runtime by defining several kernals constant data like this...
                        </p>
                            <pre><code class="has-border">#pragma kernel RowPass_128    FFTPass=RowPass_128                 FFT_RESOLUTION=128    BUTTERFLY_COUNT=7
#pragma kernel ColPass_128    FFTPass=ColPass_128   COLUMN_PASS   FFT_RESOLUTION=128    BUTTERFLY_COUNT=7
#pragma kernel RowPass_256    FFTPass=RowPass_256                 FFT_RESOLUTION=256    BUTTERFLY_COUNT=8
#pragma kernel ColPass_256    FFTPass=ColPass_256   COLUMN_PASS   FFT_RESOLUTION=256    BUTTERFLY_COUNT=8
#pragma kernel RowPass_512    FFTPass=RowPass_512                 FFT_RESOLUTION=512    BUTTERFLY_COUNT=9
#pragma kernel ColPass_512    FFTPass=ColPass_512   COLUMN_PASS   FFT_RESOLUTION=512    BUTTERFLY_COUNT=9

[numthreads(FFT_RESOLUTION, 1, 1)]
void FFTPass(uint3 id : SV_DISPATCHTHREADID)
{
    for (int i = 0; i < 8; i++)
    {
        #ifdef COLUMN_PASS
            _FourierTarget[uint3(id.xy, i)] = FFT(id.x, _FourierTarget[uint3(id.xy, i)]);
        #else
            _FourierTarget[uint3(id.yx, i)] = FFT(id.x, _FourierTarget[uint3(id.yx, i)]);
        #endif
    }
}</code></pre>
                    <p>
                        And it is likely have to employ one or several techniques to decrease the large-scale tiling into the horizen.
                        Approaches I have seen include:
                    </p>
                    <ul>
                        <li>Attenuating the displacement and normal maps with distance.</li>
                        <li>Computing a dozen or so FFT's for varying frequency bands and attenuating high frequencies with distance.</li>
                        <li>Decreasing the uv tiling with LOD bands, with blending.</li>
                    </ul>
                    </div>
                    <div class="container">
                        <a class="anchor" id="water-shading"></a>
                        <h2>Water Shading</h2>
                        <p1>
                            Time to make the surface look like a huge body of water.
                            For this, we need to simulate how light interacts with a translucent surface, meaning some light is absorbed, some is scattered and some is reflected.
                            <br>
                            ...
                            <br>
                            This is all sourced from a GDC talk on Atlas' water rendering, see <a href="https://www.youtube.com/watch?v=Dqld965-Vv0">here</a>.
                        </p1>
                    </div>
                    <div class="container">
                        <a class="anchor" id="dynamic-detail"></a>
                        <h2>Dynamic Detail</h2>
                        <p1>
                            Now we are computing wave simulation and ocean lighting, we need to render it as a collosal ocean surface.
                            So far, I have used a limited size grid mesh.
                            Simply rendering more of these is a waste of resources since the mesh does not account for the camera's perspective, which cannot perceive nearly as much detail as the geometry gets further into the skyline.
                            So I employ two methods to give the mesh extremely high detail close to the camera and extremely low detail far away.
                            To achieve this, I used two techniques:
                            <ul>
                                <li>Mesh Generation</li>
                                <li>Vertex Tessellation</li>
                            </ul>
                        </p1>
                        <p>
                            TODO: Talk about integrating DrawMeshInstancedIndirect with Vertex Tessellation.
                            <br><br>
                            The mesh shown above is 
                        </p>
                        <p>
                            Vertex Tessellation takes place inside the water surface shader, specifically in the Hull and Domain stages.
                            It allows us to recursively subdivide the triangles of our mesh, giving the vertex displacement more geometry to work with while not increasing the number of vertices sent to the GPU.
                            <br>
                            TODO: EXAMPLE GIF!
                            <br>
                            Tessellation shaders can uniformly subdivide an entire mesh, or dynamically subdivide it.
                            For this project, I increase the tessellation as we draw closer to the camera.
                            In this stage I also skip rendering triangles that are outside the camera's view planes, a practice known as frustum culling.
                            <br><br>
                            Utilizing both these methods means water near the camera can be depicted with very high detail, while keeping the total vertex count at a managable 10 000 verts.
                        </p>
                    </div>
                    <div class="container">
                        <a class="anchor" id="underwater"></a>
                        <h2>Underwater Rendering</h2>
                        <p1>
                            Did not have transparent depth prepass, so I had to do my own. Red/green face rendering etc...
                            We also need to add a case in our surface rendering function to handle the backfaces (backface not a perfect representation but it's pretty good).
                        </p1>
                    </div>
                    <div class="container">
                        <a class="anchor" id="caustics"></a>
                        <h2 id="caustics">Caustics</h2>
                        <p1>
                            'Caustics' refer to the patterns of light created when millions of light rays move from one medium to another.
                            As it does so, the light refracts - or bends - resulting in areas of high light density and low light density.
                            <br><br>
                            Many games create the appearance of caustics by simply mapping an animated texture to underwater geometry.
                            And for many games this is by far the best and easiest solution, especially those where the surface waves do not vastly change or performance is a large concern.
                            However, I want my caustic patterns to be able to reflect a calm and slow ocean surface or to a violent and stormy one and anyway in-between.
                            <br><br>
                            <a href="https://medium.com/@evanwallace/rendering-realtime-caustics-in-webgl-2a99a29a0b2c">This</a> article describes how caustic textures can be generated by refracting the vertices of a dense plane mesh, and then intersecting them against a virtual plane.
                            The color of the frag shader is then the difference in area between the non-refracted plane, and the refracted one.
                            In Unity that looks like this:
                        </p1>
                            <pre><code class="has-border">float Frag (Varyings i) : SV_Target
{
    float intialTriangleArea = length(ddx(i.originalPos)) * length(ddy(i.originalPos));
    float refractedTriangleArea = length(ddx(i.refractedPos)) * length(ddy(i.refractedPos));

    return intialTriangleArea / refractedTriangleArea;
}</code></pre>
                        <p1>
                            Which produces a texture like this:
                        </p1>
                        <figure class="image tiny">
                            <img src="../assets/projects/water-system/caustics.gif">
                            <figcaption>Caustics Texture</figcaption>
                        </figure>
                        <p1>
                            Due to the periodic nature of the FFT, we can produce a caustics texture that is seamlessly tileable across the environment.
                            In my case I used triplanar mapping.
                            <br>
                            Note that the virtual caustics plane does produce a lot of vertices when generated at a high resolution so, for me, finding the right balance of performance and detail was critical.
                        </p1>
                    </div>
                    <div class="container">
                        <a class="anchor" id="godrays"></a>
                        <h2>Godrays</h2>
                        <p1>
                            Godrays, or crepuscular rays, occur when light penetrates the water surface and is scattered outward, some of which hits the player camera.
                            We can achieve this effect by utilizing the caustics texture we just computed, but rather than mapping it to the scene, we raymarch the texture as projected by the light source:
                        </p1>
                            <pre><code class="has-border">float SampleGodrays(float3 positionWS, float3 lightDirection)
{
    float3 normal = float3(0.0, -1.0, 0.0);

    // Project caustics texture in light direction.
    float3 forward = refract(lightDirection, normal, 1.0 / IndexOfRefraction);
    float3 tangent = normalize(cross(forward, float3(0.0, 1.0, 0.0)));
    float3 bitangent = cross(tangent, forward);

    float3 sampleCoord = positionWS * _TilingFactor;
    float2 uv = float2(dot(sampleCoord, tangent), dot(sampleCoord, bitangent)) * 0.5 + 0.5;

    // Sample caustics texture at a low LOD of for some free blurring.
    // This means we can get away with a larger step size by blurring away the artefacts created.
    return SAMPLE_TEXTURE2D_LOD(_CausticsTexture, sampler_CausticsTexture, uv, 5).r;
}</code></pre>
                        <p1>
                            This function is called from a ray-marching shader, including upsampling and Gausian blur, before being composited into the final image.
                        </p1>
                        <figure>
                            <img class="has-border" src="../assets/projects/water-system/caustics_screenshot.png">
                        </figure>
                    </div>
                    <div class="container">
                        <a class="anchor" id="cutouts"></a>
                        <h2>Water Cutouts</h2>
                        <p1>
                            Ok, almost done, it is finally time to put a submarine in it.
                            To achieve this, we need some way to cull the water surface - and the volumetric effects - inside of our vessel.
                            My first idea was to render the hull of the submarine mesh using a shader that writes to the stencil buffer. Then, the surface simply skips rendering if the stencil value was written to.
                            However, while this works well for opaque surfaces, like the hull of the rowing boat in my previous ocean project, it struggles with transparent surfaces like the great glass window at the front of a submarine.
                            Handling the cutouts in screen-space was incapable of partially culling world-space volumetric effects like the godrays or solving complex cases like having multiple cutouts in a line.
                            <br>
                            We somehow need to know for any given point in world-space, if we are inside a cutout or not?
                            Enter Signed Distance Functions.
                            <br>
                            <br>
                            Signed Distance Functions, or SDF's, are kind of magical.
                            They essentially allow you to work out how far a point is from the surface of a shape, where negative values are inside the shape and positive are outside.
                            I talk more about SDF's in my Terrain Generation project but, for my water cutouts, I needed an SDF which matched the shape of the submarine's hull.
                            To achieve this, I wrote a custom SDF generator which could take a mesh and output a 3D texture where the red channel contains the distance from the surface.
                            This means that for every point in the 3D texture, we must evaluate the distance to every triangle in the mesh and store the value closest to zero.
                            <br>
                            The function for that looks like this:
                        </p1>
                        <pre><code class="has-border">float EvaluateTriangle(int triangleIndex, float3 pos)
{
    // a, b and c are the vertices of the triangle.
    float3 a = _Vertices[_Triangles[0 + triangleIndex * 3]];
    float3 b = _Vertices[_Triangles[1 + triangleIndex * 3]];
    float3 c = _Vertices[_Triangles[2 + triangleIndex * 3]];

    float3 pointOnTriangle = ClosestPointOnTriangle(pos, a, b, c);
    float3 normal = cross(b - a, c - a);
    
    float3 v = pos - pointOnTriangle;
    float3 dirToFace = normalize(v);
    float distToFace = length(v);

    if (dot(dirToFace, normal) < 0)
    {
        distToFace *= -1;
    }

    return distToFace;
}

float SmallestPointDistanceToMesh(float3 pos)
{
    float minAbsoluteDistance = 3.40282347e+38F;
    float minDistance = 3.40282347e+38F;

    for (int i = 0; i < _NumTriangles; i++)
    {
        float distance = EvaluateTriangle(i, pos);
        float absoluteDistance = abs(distance);

        if (absoluteDistance < minAbsoluteDistance)
        {
            minAbsoluteDistance = absoluteDistance;
            minDistance = distance;
        }
    }

    return minDistance;
}</code></pre>
                    <p>
                    The function I used to find the closest point on a triangle is from the <a href="https://github.com/RenderKit/embree/blob/master/tutorials/common/math/closest_point.h">Embree Ray Tracing Repo</a>.
                    <br>
                    <br>
                    These SDF's can then be given to the shader as a sort of dictionary, along with an ID and matrix the submarines in the scene creating several precise cutouts.
                    <br>
                    As an aside, sampling 3D textures can get rather expensive, so I also implemented a bounding box check which has to pass first before we sample the texture.
                    This along with frustum culling and a capped render distance makes the performance hit much more managable.
                    </p>
                    </div>
                    <div class="container" id="citations">
                        <a class="anchor" id="sources"></a>
                        <h2>Sources</h2>

                        <p>The Fourier Transform</p>
                        <a class="reference" href="https://www.thefouriertransform.com/">https://www.thefouriertransform.com/</a>

                        <p>Wakes, Explosions and Lighting: Interactive Water Simulation in Atlas</p>
                        <a href="https://www.youtube.com/watch?v=Dqld965-Vv0">https://www.youtube.com/watch?v=Dqld965-Vv0</a>

                        <p>Reflection, Refraction and Fresnel</p>
                        <a class="reference" href="https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/reflection-refraction-fresnel.html">https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-shading/reflection-refraction-fresnel.html</a>

                        <p>Ocean Simulation - antoniospg</p>
                        <a class="reference" href="https://antoniospg.github.io/UnityOcean/OceanSimulation.html">https://antoniospg.github.io/UnityOcean/OceanSimulation.html</a>

                        <p>Simulating Ocean Water - Jerry Tessendorf</p>
                        <a class="reference" href="https://people.computing.clemson.edu/~jtessen/reports/papers_files/coursenotes2004.pdf">https://people.computing.clemson.edu/~jtessen/reports/papers_files/coursenotes2004.pdf</a>

                        <p>I Tried Simulating The Entire Ocean - Acerola</p>
                        <a class="reference" href="https://www.youtube.com/watch?v=yPfagLeUa7k">https://www.youtube.com/watch?v=yPfagLeUa7k</a>

                        <p>Rendering Realtime Caustics in WebGL - Even Wallace</p>
                        <a class="reference" href="https://medium.com/@evanwallace/rendering-realtime-caustics-in-webgl-2a99a29a0b2c">https://medium.com/@evanwallace/rendering-realtime-caustics-in-webgl-2a99a29a0b2c</a>

                        <p>Periodic Caustic Textures</p>
                        <a class="reference" href="https://www.dgp.toronto.edu/public_user/stam/reality/Research/PeriodicCaustics/index.html">https://www.dgp.toronto.edu/public_user/stam/reality/Research/PeriodicCaustics/index.html</a>
                    </div>
                </div>
            </div>
        </div>
        </section>
        <!-- Article -->
        <footer id="footer"></footer>
    </body>
</html>
